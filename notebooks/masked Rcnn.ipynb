{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAeG440fB7y3"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_767SpEGD2zJ"
   },
   "source": [
    "<img src=\"https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png\" width=\"500\">\n",
    "\n",
    "**How to Train [Detectron2](https://github.com/facebookresearch/detectron2) Segmentation on a Custom Dataset**\n",
    "\n",
    "The notebook is based on official Detectron2 [colab notebook](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5) and it covers:\n",
    "- Python environment setup\n",
    "- Inference using pre-trained models\n",
    "- Download, register and visualize COCO Format Dataset\n",
    "- Configure, train and evaluate model using custom COCO Format Dataset\n",
    "\n",
    "**Preparing a Custom Dataset**\n",
    "\n",
    "In this tutorial, we will utilize an open source computer vision dataset from one of the 100,000+ available on [Roboflow Universe](https://universe.roboflow.com).\n",
    "\n",
    "If you already have your own images (and, optionally, annotations), you can convert your dataset using [Roboflow](https://roboflow.com), a set of tools developers use to build better computer vision models quickly and accurately. 150k+ developers use roboflow for (automatic) annotation, converting dataset formats (like to Detectron2), training, deploying, and improving their datasets/models.\n",
    "\n",
    "Follow [the getting started guide here](https://docs.roboflow.com/quick-start) to create and prepare your own custom dataset. Make sure to select **Instance Segmentation** Option, If you want to create your own dataset on roboflow\n",
    "\n",
    "Useful Dataset Links\n",
    "\n",
    "* [Helmet Instace Segmentation ](https://universe.roboflow.com/computer-vision-hx9i9/helmet_polygon_v2/dataset/4)\n",
    "\n",
    "* [PCB Board Instance Segmentation](https://universe.roboflow.com/chip/pcb_segmentation_yolov7/dataset/17)\n",
    "\n",
    "* [Fire Segmentation Instance Segmentation](https://universe.roboflow.com/fire-instance-segmentation/fire-detection-pr6nj/dataset/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5DwluqC5ID2"
   },
   "source": [
    "## Before you start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZBUwM3tyFWS"
   },
   "source": [
    "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator` and set it to `GPU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iil0J4nTHHb_",
    "outputId": "5cb62412-fd04-472f-cd51-c1327558c481"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MJ8SshpLaU3"
   },
   "source": [
    "## Install Detectron2 and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fM1JmUCQLdKp",
    "outputId": "b9027f79-6b5d-4eb7-ef65-d5019ff6c74b"
   },
   "outputs": [],
   "source": [
    "!python3 -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_V8w1ew59buh"
   },
   "source": [
    "Now is a good time to confirm that we have the right versions of the libraries at our disposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sqCNglJXRro5",
    "outputId": "0aa2b469-8b0b-4feb-c2ee-959cda50ea73"
   },
   "outputs": [],
   "source": [
    "import torch, detectron2\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DIEKfPKFmW54"
   },
   "outputs": [],
   "source": [
    "# COMMON LIBRARIES\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from datetime import datetime\n",
    "#from google.colab.patches import cv2_imshow\n",
    "\n",
    "# DATA SET PREPARATION AND LOADING\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "# VISUALIZATION\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "# CONFIGURATION\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "# EVALUATION\n",
    "from detectron2.engine import DefaultPredictor\n",
    "\n",
    "# TRAINING\n",
    "from detectron2.engine import DefaultTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOszeLVlErvk"
   },
   "source": [
    "## Run a Pre-trained Detectron2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-L-N75aD_hlK"
   },
   "source": [
    "Before you start training, it's a good idea to check that everything is working properly. The best way to do this is to perform inference using a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "T8sfLDV7FTYD",
    "outputId": "0e8eea7a-6407-4cb5-a7b8-6f4997bacde0"
   },
   "outputs": [],
   "source": [
    "# !wget http://images.cocodataset.org/val2017/000000439715.jpg -q -O input.jpg\n",
    "# image = cv2.imread(\"./input.jpg\")\n",
    "# cv2_imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ha0Lkah6G1NB",
    "outputId": "2d64c447-680b-4503-da9a-5e5ff055e97c"
   },
   "outputs": [],
   "source": [
    "# cfg = get_cfg()\n",
    "# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "# predictor = DefaultPredictor(cfg)\n",
    "# outputs = predictor(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NQVt8iKoJ8s_",
    "outputId": "2818d5df-4ad7-4bfc-b9a3-3aed92ea6f83"
   },
   "outputs": [],
   "source": [
    "# print(outputs[\"instances\"].pred_classes)\n",
    "# print(outputs[\"instances\"].pred_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "4fDUddSmKM-W",
    "outputId": "eb5a013e-0534-4195-9b17-4efb886ad932"
   },
   "outputs": [],
   "source": [
    "# visualizer = Visualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "# out = visualizer.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "# # cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFkJOTWvxu6G"
   },
   "source": [
    "## COCO Format Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZP3vux5vnCVn"
   },
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaohq2orBkCC"
   },
   "source": [
    "We use `football-pitch-segmentation` dataset as example. Feel free to visit [Roboflow Universe](https://universe.roboflow.com/), and select any other Instance Segmentation dataset. Make sure to download the dataset in correct - `COCO Segmentation` format.\n",
    "\n",
    "Structure of your dataset should look like this:\n",
    "\n",
    "```\n",
    "dataset-directory/\n",
    "├─ README.dataset.txt\n",
    "├─ README.roboflow.txt\n",
    "├─ train\n",
    "│  ├─ train-image-1.jpg\n",
    "│  ├─ train-image-1.jpg\n",
    "│  ├─ ...\n",
    "│  └─ _annotations.coco.json\n",
    "├─ test\n",
    "│  ├─ test-image-1.jpg\n",
    "│  ├─ test-image-1.jpg\n",
    "│  ├─ ...\n",
    "│  └─ _annotations.coco.json\n",
    "└─ valid\n",
    "   ├─ valid-image-1.jpg\n",
    "   ├─ valid-image-1.jpg\n",
    "   ├─ ...\n",
    "   └─ _annotations.coco.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the results.json file\n",
    "with open(\"/home/buddhadev/Buddhadev_Everything/OPMD/detecron2/COCO_OPMD_SP/result.json\", \"r\", encoding=\"utf-8\") as f:  # Replace with your path\n",
    "    data = json.load(f)\n",
    "\n",
    "# Group annotations by their image_id\n",
    "annotations_grouped_by_image = defaultdict(list)\n",
    "for annotation in data[\"annotations\"]:\n",
    "    annotations_grouped_by_image[annotation[\"image_id\"]].append(annotation)\n",
    "\n",
    "# Split the image data and associated annotations into train, test, and validation sets\n",
    "train_images, temp_images = train_test_split(data[\"images\"], test_size=0.3, random_state=42)\n",
    "valid_images, test_images = train_test_split(temp_images, test_size=0.5, random_state=42)\n",
    "\n",
    "# Extract annotations for each split based on the image IDs\n",
    "train_annotations = [annotations_grouped_by_image[image[\"id\"]] for image in train_images]\n",
    "train_annotations = [item for sublist in train_annotations for item in sublist]  # Flatten the list\n",
    "\n",
    "test_annotations = [annotations_grouped_by_image[image[\"id\"]] for image in test_images]\n",
    "test_annotations = [item for sublist in test_annotations for item in sublist]  # Flatten the list\n",
    "\n",
    "valid_annotations = [annotations_grouped_by_image[image[\"id\"]] for image in valid_images]\n",
    "valid_annotations = [item for sublist in valid_annotations for item in sublist]  # Flatten the list\n",
    "\n",
    "# Create directories for train, test, and validation images\n",
    "for dir_name in [\"train\", \"test\", \"valid\"]:\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "# Base path to the directory containing the images\n",
    "base_path = \"/home/buddhadev/Buddhadev_Everything/OPMD/detecron2/COCO_OPMD_SP/\"  # Adjust this path to your dataset's image directory\n",
    "\n",
    "# Move the images to their respective directories\n",
    "def move_images_to_dir(images, split_name):\n",
    "    for image_data in images:\n",
    "        source_path = os.path.join(base_path, image_data[\"file_name\"])\n",
    "        dest_path = os.path.join(split_name, os.path.basename(image_data[\"file_name\"]))\n",
    "        shutil.move(source_path, dest_path)\n",
    "\n",
    "move_images_to_dir(train_images, \"train\")\n",
    "move_images_to_dir(test_images, \"test\")\n",
    "move_images_to_dir(valid_images, \"valid\")\n",
    "\n",
    "# Save split data to JSON files\n",
    "def save_split_to_json(split_name, images, annotations):\n",
    "    split_data = {\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        \"categories\": data[\"categories\"],\n",
    "        \"info\": data[\"info\"]\n",
    "    }\n",
    "    with open(f\"{split_name}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(split_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "save_split_to_json(\"train\", train_images, train_annotations)\n",
    "save_split_to_json(\"test\", test_images, test_annotations)\n",
    "save_split_to_json(\"valid\", valid_images, valid_annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "grFIdy8ynP-7",
    "outputId": "d05be873-4ac3-4d10-d88b-fa053a263c90"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoB31yi4AoYs"
   },
   "source": [
    "### Register"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HopUGOyW853G"
   },
   "source": [
    "When you use Detectron2, before you actually train the model you need to [register it](https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html#register-a-coco-format-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbI2PNEZF3sU"
   },
   "outputs": [],
   "source": [
    "# Replace these with your dataset's name and paths\n",
    "DATA_SET_NAME = \"OPMD5\"\n",
    "ANNOTATIONS_FILE_NAME = \"result.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jntOI8GJG2ks"
   },
   "outputs": [],
   "source": [
    "# Replace the paths with your dataset's paths\n",
    "\n",
    "# TRAIN SET\n",
    "TRAIN_DATA_SET_NAME = f\"{DATA_SET_NAME}-train\"\n",
    "TRAIN_DATA_SET_IMAGES_DIR_PATH = \"/home/buddhadev/Buddhadev_Everything/OPMD/detecron2/train\"\n",
    "TRAIN_DATA_SET_ANN_FILE_PATH = \"/home/buddhadev/Buddhadev_Everything/OPMD/detecron2/train/train.json\"\n",
    "\n",
    "register_coco_instances(\n",
    "    name=TRAIN_DATA_SET_NAME,\n",
    "    metadata={},\n",
    "    json_file=TRAIN_DATA_SET_ANN_FILE_PATH,\n",
    "    image_root=TRAIN_DATA_SET_IMAGES_DIR_PATH\n",
    ")\n",
    "\n",
    "# TEST SET\n",
    "TEST_DATA_SET_NAME = f\"{DATA_SET_NAME}-test\"\n",
    "TEST_DATA_SET_IMAGES_DIR_PATH = \"/home/buddhadev/Buddhadev_Everything/OPMD/detecron2/test\"\n",
    "TEST_DATA_SET_ANN_FILE_PATH = \"/home/buddhadev/Buddhadev_Everything/OPMD/detecron2/test/test.json\"\n",
    "\n",
    "register_coco_instances(\n",
    "    name=TEST_DATA_SET_NAME,\n",
    "    metadata={},\n",
    "    json_file=TEST_DATA_SET_ANN_FILE_PATH,\n",
    "    image_root=TEST_DATA_SET_IMAGES_DIR_PATH\n",
    ")\n",
    "\n",
    "# # VALID SET\n",
    "VALID_DATA_SET_NAME = f\"{DATA_SET_NAME}-valid\"\n",
    "VALID_DATA_SET_IMAGES_DIR_PATH = \"//home/buddhadev/Buddhadev_Everything/OPMD/detecron2/valid\"\n",
    "VALID_DATA_SET_ANN_FILE_PATH = \"/home/buddhadev/Buddhadev_Everything/OPMD/detecron2/valid/valid.json\"\n",
    "\n",
    "register_coco_instances(\n",
    "    name=VALID_DATA_SET_NAME,\n",
    "    metadata={},\n",
    "    json_file=VALID_DATA_SET_ANN_FILE_PATH,\n",
    "    image_root=VALID_DATA_SET_IMAGES_DIR_PATH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOCY1UWNCtnq"
   },
   "source": [
    "We can now confirm that our custom dataset was correctly registered using [MetadataCatalog](https://detectron2.readthedocs.io/en/latest/modules/data.html#detectron2.data.MetadataCatalog)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LR8ha4EHCkA-",
    "outputId": "6506603a-3742-43ee-af5d-7f842426d25d"
   },
   "outputs": [],
   "source": [
    "[\n",
    "    data_set\n",
    "    for data_set\n",
    "    in MetadataCatalog.list()\n",
    "    if data_set.startswith(DATA_SET_NAME)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDpU2L3UL922"
   },
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4Bd_-oCA90a"
   },
   "source": [
    "Let's take a look at single entry from out train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 709
    },
    "id": "eE0anblvMGJx",
    "outputId": "b5db94b3-faf0-4c64-9717-fa906c4b76db"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "out = visualizer.draw_dataset_dict(dataset_entry)\n",
    "image_rgb = out.get_image()[:, :, ::-1]  # Convert BGR to RGB for matplotlib display\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image_rgb)\n",
    "plt.axis('off')  # No axes for this image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GavGRHy2M7Hb"
   },
   "source": [
    "## Train Model Using Custom COCO Format Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZ3g-l56NMOY"
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2 import model_zoo\n",
    "\n",
    "# SELECT THE MODEL\n",
    "MODEL_TYPE = \"mask_rcnn\"  # Choose either 'mask_rcnn' or 'faster_rcnn'\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "ARCHITECTURES = {\n",
    "    \"mask_rcnn\": \"mask_rcnn_R_101_FPN_3x\",\n",
    "    \"faster_rcnn\": \"faster_rcnn_R_101_FPN_3x\"\n",
    "}\n",
    "ARCHITECTURE = ARCHITECTURES[MODEL_TYPE]\n",
    "CONFIG_FILE_PATH = f\"COCO-InstanceSegmentation/{ARCHITECTURE}.yaml\"\n",
    "MAX_ITER = 6000\n",
    "EVAL_PERIOD = 200\n",
    "BASE_LR = 0.001\n",
    "NUM_CLASSES = 6\n",
    "\n",
    "# OUTPUT DIR\n",
    "OUTPUT_DIR_PATH = os.path.join(\n",
    "    DATA_SET_NAME,\n",
    "    ARCHITECTURE,\n",
    "    datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    ")\n",
    "os.makedirs(OUTPUT_DIR_PATH, exist_ok=True)\n",
    "\n",
    "# CONFIGURATION SETUP\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(CONFIG_FILE_PATH))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(CONFIG_FILE_PATH)\n",
    "cfg.DATASETS.TRAIN = (TRAIN_DATA_SET_NAME,)\n",
    "cfg.DATASETS.TEST = (TEST_DATA_SET_NAME,)\n",
    "\n",
    "\n",
    "augs = [\n",
    "    T.ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style=\"choice\"),\n",
    "    T.RandomFlip(prob=0.5, horizontal=True, vertical=False),\n",
    "    T.RandomFlip(prob=0.5, horizontal=False, vertical=True)\n",
    "]\n",
    "\n",
    "cfg.DATALOADER.AUGMENTATIONS = [\"ResizeShortestEdge\", \"RandomFlip\", \"RandomFlip\"]\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 32\n",
    "cfg.TEST.EVAL_PERIOD = EVAL_PERIOD\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.INPUT.MASK_FORMAT = 'bitmask'\n",
    "cfg.SOLVER.BASE_LR = BASE_LR\n",
    "cfg.SOLVER.MAX_ITER = MAX_ITER\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = NUM_CLASSES\n",
    "cfg.OUTPUT_DIR = OUTPUT_DIR_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2 import model_zoo\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "MAX_ITER = 6000\n",
    "EVAL_PERIOD = 200\n",
    "BASE_LR = 0.001\n",
    "NUM_CLASSES = 6  # Update this based on your dataset\n",
    "\n",
    "# OUTPUT DIR\n",
    "OUTPUT_DIR_PATH = os.path.join(\n",
    "    \"OPMD3_frcnn\",  # Replace with your desired output directory name\n",
    "    datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    ")\n",
    "os.makedirs(OUTPUT_DIR_PATH, exist_ok=True)\n",
    "\n",
    "# CONFIGURATION SETUP\n",
    "cfg = get_cfg()\n",
    "\n",
    "# # Set the path to your custom configuration file\n",
    "# CONFIG_FILE_PATH = \"path/to/your/config/file.yaml\"\n",
    "# cfg.merge_from_file(CONFIG_FILE_PATH)\n",
    "\n",
    "# Set the path to your downloaded model weights\n",
    "MODEL_WEIGHTS_PATH = '/home/buddhadev/Buddhadev_Everything/OPMD/Fewshot/Meta_Faster_RCNN_model_final_coco.pth'\n",
    "cfg.MODEL.WEIGHTS = MODEL_WEIGHTS_PATH\n",
    "\n",
    "# Assuming TRAIN_DATA_SET_NAME and TEST_DATA_SET_NAME are already defined\n",
    "cfg.DATASETS.TRAIN = (TRAIN_DATA_SET_NAME,)\n",
    "cfg.DATASETS.TEST = (TEST_DATA_SET_NAME,)\n",
    "\n",
    "# Augmentations (adjust as needed for your dataset)\n",
    "augs = [\n",
    "    T.ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style=\"choice\"),\n",
    "    T.RandomFlip(prob=0.5, horizontal=True, vertical=False),\n",
    "    T.RandomFlip(prob=0.5, horizontal=False, vertical=True)\n",
    "]\n",
    "cfg.DATALOADER.AUGMENTATIONS = augs\n",
    "\n",
    "# Other Configurations\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 32\n",
    "cfg.TEST.EVAL_PERIOD = EVAL_PERIOD\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.INPUT.MASK_FORMAT = 'bitmask'\n",
    "cfg.SOLVER.BASE_LR = BASE_LR\n",
    "cfg.SOLVER.MAX_ITER = MAX_ITER\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = NUM_CLASSES\n",
    "cfg.OUTPUT_DIR = OUTPUT_DIR_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krCm2L_lNC83"
   },
   "outputs": [],
   "source": [
    "# # HYPERPARAMETERS\n",
    "# ARCHITECTURE = \"mask_rcnn_R_101_FPN_3x\"\n",
    "# CONFIG_FILE_PATH = f\"COCO-InstanceSegmentation/{ARCHITECTURE}.yaml\"\n",
    "# MAX_ITER = 5000\n",
    "# EVAL_PERIOD = 200\n",
    "# BASE_LR = 0.001\n",
    "# NUM_CLASSES = 3\n",
    "\n",
    "# # OUTPUT DIR\n",
    "# OUTPUT_DIR_PATH = os.path.join(\n",
    "#     DATA_SET_NAME,\n",
    "#     ARCHITECTURE,\n",
    "#     datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "# )\n",
    "\n",
    "# os.makedirs(OUTPUT_DIR_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxQU8JrgOD73"
   },
   "outputs": [],
   "source": [
    "# cfg = get_cfg()\n",
    "# cfg.merge_from_file(model_zoo.get_config_file(CONFIG_FILE_PATH))\n",
    "# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(CONFIG_FILE_PATH)\n",
    "# cfg.DATASETS.TRAIN = (TRAIN_DATA_SET_NAME,)\n",
    "# cfg.DATASETS.TEST = (TEST_DATA_SET_NAME,)\n",
    "# cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\n",
    "# cfg.TEST.EVAL_PERIOD = EVAL_PERIOD\n",
    "# cfg.DATALOADER.NUM_WORKERS = 2\n",
    "# cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "# cfg.INPUT.MASK_FORMAT='bitmask'\n",
    "# cfg.SOLVER.BASE_LR = BASE_LR\n",
    "# cfg.SOLVER.MAX_ITER = MAX_ITER\n",
    "# cfg.MODEL.ROI_HEADS.NUM_CLASSES = NUM_CLASSES\n",
    "# cfg.OUTPUT_DIR = OUTPUT_DIR_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ch-_5aCuXWj9"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7S8y2W2AQvJq",
    "outputId": "36fe2d26-1118-4748-fff1-18a86d873970"
   },
   "outputs": [],
   "source": [
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XMPKQ28GRna"
   },
   "outputs": [],
   "source": [
    "# Look at training curves in tensorboard:\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir $OUTPUT_DIR_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flInE1L-XTfx"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsByFDFbQwLi"
   },
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "hmAcBbpXX-Rh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_valid = DatasetCatalog.get(VALID_DATA_SET_NAME)\n",
    "\n",
    "for d in dataset_valid:\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(img)\n",
    "\n",
    "    visualizer = Visualizer(\n",
    "        img[:, :, ::-1],\n",
    "        metadata=metadata,\n",
    "        scale=0.8,\n",
    "        instance_mode=ColorMode.IMAGE_BW\n",
    "    )\n",
    "    out = visualizer.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYGk-zJz4mQF"
   },
   "outputs": [],
   "source": [
    "dataset_valid = DatasetCatalog.get(VALID_DATA_SET_NAME)\n",
    "\n",
    "for d in dataset_valid:\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(img)\n",
    "\n",
    "    visualizer = Visualizer(\n",
    "        img[:, :, ::-1],\n",
    "        metadata=metadata,\n",
    "        scale=0.8,\n",
    "        instance_mode=ColorMode.IMAGE_BW\n",
    "    )\n",
    "    out = visualizer.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from detectron2.data import DatasetCatalog\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "\n",
    "# Assuming you've already registered your dataset and set VALID_DATA_SET_NAME\n",
    "dataset_valid = DatasetCatalog.get(VALID_DATA_SET_NAME)\n",
    "\n",
    "for d in dataset_valid:\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(img)\n",
    "\n",
    "    visualizer = Visualizer(\n",
    "        img[:, :, ::-1],\n",
    "        metadata=metadata,  # Ensure this is set to your dataset's metadata\n",
    "        scale=0.8,\n",
    "        instance_mode=ColorMode.IMAGE_BW  # ColorMode.IMAGE_BW or other modes you prefer\n",
    "    )\n",
    "    out = visualizer.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    \n",
    "    # Display the image in Jupyter Notebook\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(out.get_image()[:, :, ::-1])\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from detectron2.data import DatasetCatalog\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "\n",
    "# Assuming you've already registered your dataset and set TEST_DATA_SET_NAME\n",
    "dataset_test = DatasetCatalog.get(TEST_DATA_SET_NAME)\n",
    "\n",
    "for d in dataset_test:\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(img)\n",
    "\n",
    "    visualizer = Visualizer(\n",
    "        img[:, :, ::-1],\n",
    "        metadata=metadata,  # Ensure this is set to your dataset's metadata\n",
    "        scale=0.8,\n",
    "        instance_mode=ColorMode.IMAGE_BW  # ColorMode.IMAGE_BW or other modes you prefer\n",
    "    )\n",
    "    out = visualizer.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    \n",
    "    # Display the image in Jupyter Notebook\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(out.get_image()[:, :, ::-1])\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the modified code to save the visualized images in your local environment:\n",
    "\n",
    "SAVE_DIR = \"./visualized_predictions3/\"  # Define where you want to save the visualized images\n",
    "\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "\n",
    "for idx, d in enumerate(dataset_test):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(img)\n",
    "\n",
    "    visualizer = Visualizer(\n",
    "        img[:, :, ::-1],\n",
    "        metadata=metadata,  # Ensure this is set to your dataset's metadata\n",
    "        scale=0.8,\n",
    "        instance_mode=ColorMode.IMAGE_BW  # ColorMode.IMAGE_BW or other modes you prefer\n",
    "    )\n",
    "    out = visualizer.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "    # Display the image using matplotlib\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(out.get_image()[:, :, ::-1])\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Save the visualized image using matplotlib\n",
    "    save_path = os.path.join(SAVE_DIR, f\"visualized_{idx}.jpg\")\n",
    "    plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()  # Close the figure after saving\n",
    "\n",
    "# Note: You can run the provided code on your local machine where you have Detectron2 and the necessary datasets. \n",
    "# This will save the visualized images using matplotlib to the specified directory (in this case, \"./visualized_predictions/\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"/home/buddhadev/Buddhadev_Everything/OPMD/detecron2/OPMD3_fSL/2023-11-15-21-11-08/model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.50\n",
    "predictor = DefaultPredictor(cfg)\n",
    "evaluator = COCOEvaluator(\"OPMD5-test\", cfg, False, output_dir=\"./output/\")\n",
    "val_loader = build_detection_test_loader(cfg, \"OPMD5-test\")\n",
    "inference_on_dataset(trainer.model, val_loader, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
